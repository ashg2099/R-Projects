{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9V5ptJAIEbw"
   },
   "source": [
    "# FIT5197 2024 S1 Final Assessment\n",
    "\n",
    "**SPECIAL NOTE:** Please refer to the [assessment page](https://learning.monash.edu/mod/assign/view.php?id=2017255) for rules, general guidelines and marking rubrics of the assessment (the marking rubric for the kaggle competition part will be released near the deadline in the same page). Failure to comply with the provided information will result in a deduction of mark (e.g., late penalties) or breach of academic integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkfAwCwbg7is"
   },
   "source": [
    "**YOUR NAME**: Ashwin Gururaj\n",
    "\n",
    "**STUDENT ID**: 33921199\n",
    "\n",
    "**KAGGLE NAME/ID**: Ashwin Gururaj - ashwing2099\n",
    "\n",
    "Please also enter your details in this [google form](https://forms.gle/isxqfrVnV7ddAj8y8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_u6Dyj1xehk"
   },
   "source": [
    "# Part 1 Regression (50 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "marked-instrument"
   },
   "source": [
    "A few thousand people were questioned in a [life and wellbeing survey](https://www.get-happier.com/) to build a model to predict happiness of an individual. You need to build regression models to optimally predict the variable in the survey dataset called 'happiness' based on any, or all, of the other survey question responses. \n",
    "\n",
    "You have been provided with two datasets, ```regression_train.csv``` and ```regression_test.csv```. Using these datasets, you hope to build a model that can predict happiness level using the other variables. ```regression_train.csv``` comes with the ground-truth target label (i.e. happiness level) whereas `regression_test.csv` comes with independent variables (input information) only.\n",
    "\n",
    "On the order of around 70 survey questions have been converted into predictor variables that can be used to predict happiness. We do not list all the predictor names here, but their names given in the data header can clearly be linked to the survey questions. e.g., the predictor variable 'iDontFeelParticularlyPleasedWithTheWayIAm' corresponds to the survey question 'I don’t feel particularly pleased with the way I am.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-Gbtyt1g7iv"
   },
   "source": [
    "**PLEASE NOTE THAT THE USE OF LIBRARIES ARE PROHIBITED IN THESE QUESTIONS UNLESS STATED OTHERWISE, ANSWERS USING LIBRARIES WILL RECEIVE 0 MARKS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wound-marriage"
   },
   "source": [
    "## Question 1 (NO LIBRARIES ALLOWED) (4 Mark)\n",
    "Please load the ```regression_train.csv``` and fit a [$\\textbf{multiple linear regression model}$](https://en.wikipedia.org/wiki/Linear_regression) with 'happiness' being the target variable. According to the summary table, which predictors do you think are possibly associated with the target variable (use the significance level of 0.01), and which are the **Top 5** strongest predictors? Please write an R script to automatically fetch and print this information.\n",
    "\n",
    "**NOTE**: Manually doing the above tasks will result in 0 marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ANSWER BLOCK\n",
    "# Ensure the file 'regression_train.csv' is in the working directory\n",
    "regression_train_df <- read.csv('regression_train.csv')\n",
    "\n",
    "# Create the multiple linear regression model\n",
    "multiple_linear_regresion_model <- lm(happiness ~ ., data=regression_train_df)\n",
    "\n",
    "# Obtain the summary statistics for the regression model\n",
    "model_summary <- summary(multiple_linear_regresion_model)\n",
    "\n",
    "# Extract the coefficients table from the model summary\n",
    "coefficients_summary_table <- model_summary$coefficients\n",
    "\n",
    "# Select predictors with p-values less than 0.01, indicating strong evidence against the null hypothesis\n",
    "significant_predictors <- coefficients_summary_table[coefficients_summary_table[,4] < 0.01, ]\n",
    "\n",
    "# Determine the top 5 most influential predictors based on the absolute value of the t-statistic\n",
    "top_5_most_significant_predictors <- head(significant_predictors[order(abs(significant_predictors[,3]), decreasing=TRUE), ], 5)\n",
    "\n",
    "# Display the significant predictors\n",
    "cat(\"Significant Predictors (p-value < 0.01):\\n\")\n",
    "print(significant_predictors)\n",
    "\n",
    "# Display the top 5 most influential predictors\n",
    "cat(\"\\nTop 5 Strongest Predictors:\\n\")\n",
    "print(top_5_most_significant_predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "The objective of this analysis is to identify significant predictors of happiness and determine the strongest influencers among them. By fitting a multiple linear regression model, we found that income-related variables are the most significant predictors of happiness, with higher income brackets showing stronger associations with increased happiness levels. The top five predictors, based on their statistical significance and t-values, are different income categories, highlighting the impact of financial well-being on happiness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "royal-finland"
   },
   "source": [
    "## Question 2 (2 Mark)\n",
    "[**R squared**](https://en.wikipedia.org/wiki/Coefficient_of_determination) from the summary table reflects that the full model doesn't fit the training dataset well; thus, you try to quantify the error between the values of the ground-truth and those of the model prediction. You want to write a function to predict 'happiness' with the given dataset and calculate the [root mean squared error (rMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation) between the model predictions and the ground truths. Please test this function on the full model and the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ANSWER BLOCK\n",
    "# Computing the Root Mean Squared Error (rMSE)\n",
    "compute_rmse <- function(observed_values, predicted_values) {\n",
    "  sqrt(mean((observed_values - predicted_values)^2))\n",
    "}\n",
    "\n",
    "# Load data file\n",
    "survey_train_data <- read.csv(\"regression_train.csv\")\n",
    "\n",
    "# Build a linear regression model using all significant predictors to predict 'happiness'\n",
    "happiness_model <- lm(happiness ~ ., data = survey_train_data)\n",
    "\n",
    "# Generate predictions on the training dataset\n",
    "happiness_predictions <- predict(happiness_model, survey_train_data)\n",
    "\n",
    "# Calculate the (rMSE) between actual and predicted happiness levels\n",
    "happiness_rmse <- compute_rmse(survey_train_data$happiness, happiness_predictions)\n",
    "\n",
    "# Print the computed rMSE\n",
    "print(paste(\"Computed Root Mean Squared Error (rMSE):\", happiness_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation \n",
    "The objective of this analysis is to evaluate the accuracy of the multiple linear regression model in predicting happiness by calculating the root mean squared error (rMSE). The computed rMSE value of 6.67 indicates the average magnitude of error between the predicted and actual happiness levels, reflecting the model's performance on the training data. This metric helps quantify the model's predictive accuracy and identify areas for potential improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eight-newman"
   },
   "source": [
    "## Question 3 (2 Marks)\n",
    "You find the full model complicated and try to reduce the complexity by performing [bidirectional stepwise regression](https://en.wikipedia.org/wiki/Stepwise_regression) with [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion).\n",
    "\n",
    "Calculate the **rMSE** of this new model with the function that you implemented previously. Is there anything you find unusual? Explain your findings in 100 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ANSWER BLOCK\n",
    "# Perform bidirectional stepwise regression using BIC to simplify the model\n",
    "simplified_model <- step(happiness_model, direction = \"both\", k = log(nrow(survey_train_data)), trace = FALSE)\n",
    "\n",
    "# Predict 'happiness' values using the simplified stepwise regression model\n",
    "simplified_predictions <- predict(simplified_model, newdata = survey_train_data)\n",
    "\n",
    "# Calculate the root mean squared error (rMSE) between the predicted and actual 'happiness' values\n",
    "observed_happiness <- survey_train_data$happiness\n",
    "rmse_simplified <- compute_rmse(observed_happiness, simplified_predictions)\n",
    "\n",
    "# Print the calculated rMSE for the simplified stepwise regression model\n",
    "print(paste(\"Root Mean Squared Error (rMSE) for Simplified Model:\", rmse_simplified))\n",
    "\n",
    "# Findings explanation\n",
    "cat(\"\\nExplanation of Findings:\\n\")\n",
    "cat(\"The initial full model had an RMSE of approximately 6.67, which indicates the average error between the predicted and actual happiness levels using all predictors. After performing bidirectional stepwise regression using BIC, the RMSE of the new model is 7.33. This suggests that the stepwise model, despite having fewer predictors, has a slightly higher average error compared to the full model. Although the stepwise model is more streamlined and easier to interpret, it comes at the cost of a minor increase in predictive error. This trade-off between model complexity and accuracy is common in regression modeling. The stepwise model may still be preferable in situations where model simplicity and interpretability are prioritized over a slight increase in prediction error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "latter-translation"
   },
   "source": [
    "## Question 4 (2 Mark)\n",
    "Although stepwise regression has reduced the model complexity significantly, the model still contains a lot of variables that we want to remove. Therefore, you are interested in lightweight linear regression models with ONLY TWO predictors. Write a script to automatically find the best lightweight model which corresponds to the model with the least **rMSE** on the training dataset. Compare the **rMSE** of the best lightweight model with the **rMSE** of the full model - ```lm.fit``` - that you built previously. Give an explanation for these results based on consideration of the predictors involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# ANSWER BLOCK\n",
    "# Ensure the file 'regression_train.csv' is in the working directory\n",
    "regression_train_df <- read.csv('regression_train.csv')\n",
    "\n",
    "# Create the multiple linear regression model\n",
    "multiple_linear_regresion_model <- lm(happiness ~ ., data=regression_train_df)\n",
    "\n",
    "# Function to predict 'happiness' and calculate RMSE using the initially fitted 'lm' model\n",
    "calculate_rmse <- function(model, dataset) {\n",
    "  # Predict 'happiness' using the 'lm' model and dataset\n",
    "  predictions <- predict(model, newdata = dataset)\n",
    "  \n",
    "  # Calculate the residuals (errors) between predictions and actual values\n",
    "  residuals <- dataset$happiness - predictions\n",
    "  \n",
    "  # Calculate the RMSE\n",
    "  rmse <- sqrt(mean(residuals^2))\n",
    "  \n",
    "  return(rmse)\n",
    "}\n",
    "\n",
    "# Calculate RMSE for the full model\n",
    "rmse_initial <- calculate_rmse(multiple_linear_regresion_model, regression_train_df)\n",
    "cat(\"Root Mean Squared Error (RMSE) on the training dataset for the full model:\", rmse_initial, \"\\n\")\n",
    "\n",
    "# Find the best lightweight model with only two predictors\n",
    "predictors <- names(regression_train_df)[names(regression_train_df) != \"happiness\"]\n",
    "best_rmse <- Inf\n",
    "best_model <- NULL\n",
    "best_predictors <- NULL\n",
    "\n",
    "for (i in 1:(length(predictors) - 1)) {\n",
    "  for (j in (i + 1):length(predictors)) {\n",
    "    formula <- as.formula(paste(\"happiness ~\", predictors[i], \"+\", predictors[j]))\n",
    "    model <- lm(formula, data=regression_train_df)\n",
    "    rmse <- calculate_rmse(model, regression_train_df)\n",
    "    if (rmse < best_rmse) {\n",
    "      best_rmse <- rmse\n",
    "      best_model <- model\n",
    "      best_predictors <- c(predictors[i], predictors[j])\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Print the best lightweight model predictors and its RMSE\n",
    "cat(\"Best lightweight model predictors:\", best_predictors, \"\\n\")\n",
    "cat(\"Root Mean Squared Error (RMSE) of the best lightweight model on the training dataset:\", best_rmse, \"\\n\")\n",
    "\n",
    "# Explanation of findings\n",
    "cat(\"\\nExplanation of Findings:\\n\")\n",
    "cat(\"The RMSE of the full model is approximately\", rmse_initial, \", while the RMSE of the best lightweight model with only two predictors is\", best_rmse, \". The best lightweight model has a higher RMSE compared to the full model, which is expected since the full model uses all available predictors. Although the lightweight model is simpler and easier to interpret, it comes with a minor increase in predictive error. This trade-off between model complexity and accuracy is common in regression modeling. The lightweight model may still be preferable when interpretability and simplicity are more important than a slight increase in prediction error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4P9QL6tg7ix"
   },
   "source": [
    "### ANSWER (TEXT)\n",
    "The RMSE of the full model is approximately 6.672557 , while the RMSE of the best lightweight model with only two predictors is 7.885411 . The best lightweight model has a higher RMSE compared to the full model, which is expected since the full model uses all available predictors. Although the lightweight model is simpler and easier to interpret, it comes with a minor increase in predictive error. This trade-off between model complexity and accuracy is common in regression modeling. The lightweight model may still be preferable when interpretability and simplicity are more important than a slight increase in prediction error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preceding-chicago"
   },
   "source": [
    "## Question 5 (Libraries are allowed) (40 Marks)\n",
    "As a Data Scientist, one of the key tasks is to build models $\\textbf{most appropriate/closest}$ to the truth; thus, modelling will not be limited to the aforementioned steps in this assignment. To simulate for a realistic modelling process, this question will be in the form of a [Kaggle competition](https://www.kaggle.com/t/ad8c96e412254c138cbec1d9d1c09734) among students to find out who has the best model.\n",
    "\n",
    "Thus, you **will be graded** by the **rMSE** performance of your model, the better your model, the higher your score. Additionally, you need to describe/document your thought process in this model building process, this is akin to showing your working properly for the mathematic sections. If you don't clearly document the reasonings behind the model you use, we will have to make some deductions on your scores.\n",
    "\n",
    "This is the [video tutorial](https://www.youtube.com/watch?v=rkXc25Uvyl4) on how to join any Kaggle competition. \n",
    "\n",
    "When you optimize your model's performance, you can use any supervised model that you know and feature selection might be a big help as well. [Check the non-exhaustive set of R functions relevant to this unit](https://learning.monash.edu/mod/resource/view.php?id=2017193) for ideas for different models to try.\n",
    "\n",
    "$\\textbf{Note}$ Please make sure that we can install the libraries that you use in this part, the code structure can be:\n",
    "\n",
    "```install.packages(\"some package\", repos='http://cran.us.r-project.org')```\n",
    "\n",
    "```library(\"some package\")```\n",
    "\n",
    "Remember that if we cannot run your code, we will have to give you a deduction. Our suggestion is for you to use the standard ```R version 3.6.1```\n",
    "\n",
    "You also need to name your final model ``fin.mod`` so we can run a check to find out your performance. A good test for your understanding would be to set the previous $\\textbf{BIC model}$ to be the final model to check if your code works perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8x-H3d66g7ix",
    "scrolled": false,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Install and load required libraries\n",
    "install.packages(\"tidyverse\", repos='http://cran.us.r-project.org')\n",
    "install.packages(\"caret\", repos='http://cran.us.r-project.org')\n",
    "install.packages(\"xgboost\", repos='http://cran.us.r-project.org')\n",
    "\n",
    "library(tidyverse)\n",
    "library(caret)\n",
    "library(xgboost)\n",
    "\n",
    "# Import the dataset\n",
    "training_data <- read.csv('regression_train.csv')\n",
    "testing_data <- read.csv('regression_test.csv')\n",
    "\n",
    "# Identify any missing values\n",
    "sum(is.na(training_data))\n",
    "\n",
    "# Divide the data into training and validation sets\n",
    "set.seed(123)\n",
    "training_indices <- createDataPartition(training_data$happiness, p = .8, \n",
    "                                        list = FALSE, \n",
    "                                        times = 1)\n",
    "train_set <- training_data[training_indices,]\n",
    "validation_set <- training_data[-training_indices,]\n",
    "\n",
    "# Define a control function for cross-validation\n",
    "cv_control <- trainControl(method=\"cv\", number=5)\n",
    "\n",
    "# Focused hyperparameter grid for XGBoost\n",
    "xgb_hyper_grid <- expand.grid(\n",
    "  nrounds = c(100, 200),\n",
    "  max_depth = c(6, 8),\n",
    "  eta = c(0.01, 0.1),\n",
    "  gamma = c(0, 1),\n",
    "  colsample_bytree = c(0.6, 0.8),\n",
    "  min_child_weight = c(1, 5),\n",
    "  subsample = c(0.8, 1)\n",
    ")\n",
    "\n",
    "# Additional parameters for regularization\n",
    "xgb_regularization_params <- list(\n",
    "  alpha = 1,  # L1 regularization term\n",
    "  lambda = 1  # L2 regularization term\n",
    ")\n",
    "\n",
    "# Train an XGBoost model with focused hyperparameter tuning\n",
    "set.seed(123)\n",
    "xgb_tuned_model <- train(happiness ~ ., data=train_set, method=\"xgbTree\", \n",
    "                         trControl=cv_control, tuneGrid=xgb_hyper_grid,\n",
    "                         alpha = xgb_regularization_params$alpha, lambda = xgb_regularization_params$lambda)\n",
    "\n",
    "# Finalize the XGBoost model using the entire training dataset\n",
    "fin.mob_xgb_model <- train(happiness ~ ., data=training_data, method=xgb_tuned_model$method, \n",
    "                         trControl=cv_control, tuneGrid=xgb_tuned_model$bestTune,\n",
    "                         alpha = xgb_regularization_params$alpha, lambda = xgb_regularization_params$lambda)\n",
    "\n",
    "# Compute RMSE on the validation set for XGBoost\n",
    "xgb_val_predictions <- predict(fin.mob_xgb_model, newdata=validation_set)\n",
    "xgb_val_rmse <- sqrt(mean((validation_set$happiness - xgb_val_predictions)^2))\n",
    "cat(\"Validation RMSE (XGBoost):\", xgb_val_rmse, \"\\n\")\n",
    "\n",
    "# Compute RMSE on the full training set for XGBoost\n",
    "xgb_train_predictions <- predict(fin.mob_xgb_model, newdata=training_data)\n",
    "xgb_train_rmse <- sqrt(mean((training_data$happiness - xgb_train_predictions)^2))\n",
    "cat(\"Training RMSE (XGBoost):\", xgb_train_rmse, \"\\n\")\n",
    "\n",
    "# Final model summary for XGBoost\n",
    "print(summary(fin.mob_xgb_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation and Objective\n",
    "The objective of this analysis is to optimize the model for predicting happiness by using XGBoost with hyperparameter tuning. The process involves splitting the data into training and validation sets, performing cross-validation, and adjusting regularization parameters to prevent overfitting. The model's performance is evaluated using the root mean squared error (rMSE) on both the validation and training datasets.\n",
    "\n",
    "### Model usage justification\n",
    "XGBoost was chosen for its robustness and efficiency in handling large datasets with numerous features. It provides a sophisticated approach to model boosting, which often yields high predictive accuracy. The model's ability to perform automatic feature selection, handle missing data, and its scalability make it suitable for this complex task. While other models like linear regression or random forests were considered, XGBoost's superior performance in various Kaggle competitions and real-world applications justified its selection as the primary model for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0SYx-zYg7ix",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Load in the test data.\n",
    "test <- read.csv(\"regression_test.csv\")\n",
    "# If you are using any packages that perform the prediction differently, please change this line of code accordingly.\n",
    "pred.label <- predict(fin.mob_xgb_model, test)\n",
    "# put these predicted labels in a csv file that you can use to commit to the Kaggle Leaderboard\n",
    "write.csv(\n",
    "    data.frame(\"RowIndex\" = seq(1, length(pred.label)), \"Prediction\" = pred.label),  \n",
    "    \"RegressionPredictLabel.csv\", \n",
    "    row.names = F\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLEASE DO NOT ALTER THIS CODE BLOCK, YOU ARE REQUIRED TO HAVE THIS CODE BLOCK IN YOUR JUPYTER NOTEBOOK SUBMISSION\n",
    "## Please skip (don't run) this if you are a student\n",
    "## For teaching team use only\n",
    "\n",
    "tryCatch(\n",
    "    {\n",
    "        source(\"../supplimentary.R\")\n",
    "    },\n",
    "    error = function(e){\n",
    "        source(\"supplimentary.R\")\n",
    "    }\n",
    ")\n",
    "\n",
    "truths <- tryCatch(\n",
    "    {\n",
    "        read.csv(\"../regression_test_label.csv\")\n",
    "    },\n",
    "    error = function(e){\n",
    "        read.csv(\"regression_test_label.csv\")\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "RMSE.fin <- rmse(pred.label, truths$x)\n",
    "cat(paste(\"RMSE is\", RMSE.fin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Classification (50 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few thousand people were questioned in a [life and wellbeing survey](https://www.get-happier.com/) to build a model to predict happiness of an individual, but this time we want to predict a categorical score for perfect mental health, rather than a continuous score. You need to build 5-class classification models to optimally predict the variable in the survey dataset called 'perfectMentalHealth' based on any, or all, of the other survey question responses. \n",
    "\n",
    "You have been provided with two datasets, ```classification_train.csv``` and ```classification_test.csv```. Using these datasets, you hope to build a model that can predict 'perfectMentalHealth' using the other variables. ```classification_train.csv``` comes with the ground-truth target label (i.e. 'perfectMentalHealth' happiness classes) whereas `classification_test.csv` comes with independent variables (input information) only.\n",
    "\n",
    "On the order of around 70 survey questions have been converted into predictor variables that can be used to predict 'perfectMentalHealth'. We do not list all the predictor names here, but their names given in the data header can clearly be linked to the survey questions. E.g. the predictor variable 'iDontFeelParticularlyPleasedWithTheWayIAm' corresponds to the survey question 'I don’t feel particularly pleased with the way I am.'\n",
    "\n",
    "This question will also be in the form of a [Kaggle competition](https://www.kaggle.com/t/968d3b346acb47779771f47785c39e62) among students to find out who has the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "install.packages(\"tidyverse\", repos='http://cran.us.r-project.org')\n",
    "install.packages(\"caret\", repos='http://cran.us.r-project.org')\n",
    "install.packages(\"randomForest\", repos='http://cran.us.r-project.org')\n",
    "install.packages(\"doParallel\", repos='http://cran.us.r-project.org')\n",
    "\n",
    "# Import necessary libraries\n",
    "library(tidyverse)\n",
    "library(caret)\n",
    "library(randomForest)\n",
    "library(doParallel)\n",
    "\n",
    "# Read the datasets for training and testing\n",
    "train_dataset <- read.csv('classification_train.csv')\n",
    "test_dataset <- read.csv('classification_test.csv')\n",
    "\n",
    "# Change the target var to a factor\n",
    "train_dataset$perfectMentalHealth <- as.factor(train_dataset$perfectMentalHealth)\n",
    "\n",
    "# Save the original factor levels for mapping predictions back\n",
    "original_factor_levels <- levels(train_dataset$perfectMentalHealth)\n",
    "\n",
    "# Ensure factor levels are valid R variable names\n",
    "levels(train_dataset$perfectMentalHealth) <- make.names(levels(train_dataset$perfectMentalHealth))\n",
    "\n",
    "# Check for missing values and remove any if found\n",
    "missing_values <- sum(is.na(train_dataset))\n",
    "train_dataset <- na.omit(train_dataset)\n",
    "\n",
    "# Split the complet data into training and validation sets\n",
    "set.seed(123)\n",
    "index_train <- createDataPartition(train_dataset$perfectMentalHealth, p = 0.8, \n",
    "                                   list = FALSE, \n",
    "                                   times = 1)\n",
    "train_subset <- train_dataset[index_train,]\n",
    "validation_subset <- train_dataset[-index_train,]\n",
    "\n",
    "# Ensure factor levels are consistent between training and validation sets\n",
    "validation_subset$perfectMentalHealth <- factor(validation_subset$perfectMentalHealth, levels = levels(train_subset$perfectMentalHealth))\n",
    "\n",
    "# Set up cross-validation control\n",
    "cross_val_control <- trainControl(method = \"cv\", number = 10, classProbs = TRUE, summaryFunction = multiClassSummary)\n",
    "\n",
    "# Customised grid for Random Forest model's hyperparameter tuning\n",
    "rf_param_grid <- expand.grid(\n",
    "  mtry = c(2, 4, 6, 8, 10)\n",
    ")\n",
    "\n",
    "# Perform feature selection using Recursive Feature Elimination (RFE)\n",
    "set.seed(123)\n",
    "rfe_ctrl <- rfeControl(functions = rfFuncs, method = \"cv\", number = 20, verbose = FALSE)\n",
    "rfe_output <- rfe(train_subset[, -which(names(train_subset) == \"perfectMentalHealth\")], train_subset$perfectMentalHealth, \n",
    "                  sizes = c(1:20), rfeControl = rfe_ctrl)\n",
    "\n",
    "# Extract the selected features\n",
    "important_features <- predictors(rfe_output)\n",
    "print(important_features)\n",
    "\n",
    "# Enable parallel processing to speed up model training\n",
    "parallel_cluster <- makePSOCKcluster(detectCores() - 1)\n",
    "registerDoParallel(parallel_cluster)\n",
    "\n",
    "# Train the Random Forest model with selected features and hyperparameter tuning\n",
    "set.seed(123)\n",
    "trained_rf_model <- train(\n",
    "  form = as.formula(paste(\"perfectMentalHealth ~\", paste(important_features, collapse = \" + \"))),\n",
    "  data = train_subset,\n",
    "  method = \"rf\",\n",
    "  trControl = cross_val_control,\n",
    "  tuneGrid = rf_param_grid,\n",
    "  ntree = 1000\n",
    ")\n",
    "\n",
    "# Disable parallel processing\n",
    "stopCluster(parallel_cluster)\n",
    "registerDoSEQ()\n",
    "\n",
    "# Concluding the model using the complete training dataset\n",
    "final_rf_model <- randomForest(as.formula(paste(\"perfectMentalHealth ~\", paste(important_features, collapse = \" + \"))), \n",
    "                               data = train_dataset, mtry = trained_rf_model$bestTune$mtry, ntree = 500)\n",
    "\n",
    "# Calculate the validation dataset accuracy metric\n",
    "val_dataset_predicts <- predict(final_rf_model, newdata = validation_subset)\n",
    "conf_mat <- confusionMatrix(val_dataset_predicts, validation_subset$perfectMentalHealth)\n",
    "val_acc <- conf_mat$overall['Accuracy']\n",
    "cat(\"Validation Accuracy (Random Forest):\", val_acc, \"\\n\")\n",
    "\n",
    "# Print summary of the final Random Forest model\n",
    "print(summary(final_rf_model))\n",
    "\n",
    "# Generate predicts on the test set\n",
    "test_set_predicts <- predict(final_rf_model, newdata = test_dataset)\n",
    "\n",
    "# Map the predictions back to the original factor levels\n",
    "final_predictions <- original_factor_levels[as.numeric(test_set_predicts)]\n",
    "\n",
    "# Update the results to the csv file\n",
    "write.csv(\n",
    "  data.frame(\"RowIndex\" = seq(1, length(final_predictions)), \"Prediction\" = final_predictions),  \n",
    "  \"ClassificationPredictLabel_RF.csv\", \n",
    "  row.names = FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation \n",
    "The objective of this analysis is to build a robust classification model to predict the 'perfectMentalHealth' category using survey responses. The approach involves data preprocessing, feature selection, and model tuning to enhance the accuracy of the predictions. The Random Forest model was chosen due to its efficiency in handling high-dimensional data and its ability to perform feature selection automatically, leading to improved model performance and interpretability. \n",
    "To ensure the model's performance is thoroughly tested and results are reproducible, we used the set.seed function. This ensures that the random processes such as data splitting and model training yield consistent results across different runs, making our model's performance reliable and comparable. By setting a seed, we can confirm that the model is robust and that its performance metrics, such as accuracy, are stable and not subject to random variations.\n",
    "\n",
    "### Model usage justification\n",
    "Random Forest was selected for this task because of its proven effectiveness in dealing with complex, high-dimensional datasets. It handles both continuous and categorical variables well and is less prone to overfitting compared to other models. Additionally, Random Forests can provide insights into feature importance, making it a powerful tool for this multi-class classification problem. While other models like SVM or neural networks could be used, Random Forest offers a good balance of accuracy, interpretability, and ease of implementation, making it the preferred choice for this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gross-disaster",
    "outputId": "ac8b1b5a-ad80-42f7-824a-61432e4d64cf",
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## PLEASE DO NOT ALTER THIS CODE BLOCK, YOU ARE REQUIRED TO HAVE THIS CODE BLOCK IN YOUR JUPYTER NOTEBOOK SUBMISSION\n",
    "## Please skip (don't run) this if you are a student\n",
    "## For teaching team use only\n",
    "\n",
    "truths <- tryCatch(\n",
    "    {\n",
    "        read.csv(\"../classification_test_label.csv\")\n",
    "    },\n",
    "    error = function(e){\n",
    "        read.csv(\"classification_test_label.csv\")\n",
    "    }\n",
    ")\n",
    "\n",
    "f1_score <- F1_Score(truths$x, pred.label)\n",
    "cat(paste(\"f1_score is\", f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FIT5197_Assignment_Marking_Guidelines.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
